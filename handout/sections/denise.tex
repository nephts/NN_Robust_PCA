\section{Denise}\label{sec:algorithm}

We consider $\mathbb{S}_n \subseteq \mathbb{R}^{n \times n}$ to be the set of $n$-by-$n$ symmetric matrices and $P_n\subseteq \mathbb{S}_n$ to be the subset of positive semi-definite matrices and $P_{k,n} \subseteq P_n$ the subset of matrices with rank at most $k$. As the input of our deep neuronal network we consider a Matrix $M = [M_{i,j}]_{i,j} \in P_n$. $M$ is to be decomposed as a sum $M = L + S$ where $L = [L_{i,j}]_{i,j} \in P_{k,n}$ is of rank at most $k$ and a sparse matrix $S = [S_{i,j}]_{i,j} \in P_n$. Since $L$ is assumed to be symmetric, by the Cholesky decomposition, we can represent it as $L=UU^T$, where $U = [U_{i,j}]_{i,j} \in \mathbb{R}^{n \times k}$. Therefore $M$ can be expressed as $M = UU^T + S$.

\paragraph{Loss function}
We input $M$ in the neural network, which should faithfully output the low rank matrix $U$. Hence, we want to minimize the difference between $M$ and $UU^T$, which is equal to $S$. A convenient choice of loss-function for the considered neural network is, therefore, given by the $l_1$-matrix-norm of $S$
\[
\Vert UU^T - M \Vert_{l_1} = \Vert S \Vert_{l_1}
\]
The $l_1$-norm is a common choice to guarantee sparsity of $S$.

\paragraph{Architecture}
As the matrix M is symmetric, we can reduce the input from $n^2$ to $n(n + 1)/2$ by taking the triangular lower matrix of $M$. The lower matrix is then transformed into a vector using the operator h:
\[
h: S^n \to \mathbb{R}^{n(n+1)/2}, \, M \mapsto (M_{1,1},M_{2,1},M_{2,2},\dots,M_{n,1},\dots,M_{n,n})^T
\]
Similarly we convert the output vector of the neural network into a matrix with the operator g defined as
\[
g : \mathbb{R}^{nk} \to \mathbb{R}^{n \times k}, \, X \mapsto \begin{pmatrix} X_1 & \cdots & X_k \\ \vdots & & \vdots \\ X_{(n-1)k + 1} & \cdots& X_{(n-1)k+k}\end{pmatrix}
\]
Besides the output layer, our multi-layer feed-forward neural network $\mathcal{N}: \mathbb{R}^{n(n+1)/2} \to \mathbb{R}^{nk} $ has three hidden dense layers, each exhibiting ReLU-activation function and $n/2$ nodes. Using $h$ and $g$ the matrix $U$ is the output of the neural network $U = g(\mathcal{N}(h(M)))$ and we  get the desired matrix $L=\rho(\mathcal{N}(h(M)))$ for
\[
\rho : \mathbb{R}^{rd} \to P_{r,d}, X \mapsto g(X)g(X)^T
\]


\paragraph{Generation of training data}
For the training of Denise a synthetic dataset consisting of randomly generated matrices with appropriate decomposition properties is created. In detail, we construct a sample of positive semidefinite $n$-by-$n$ matrices $M$ that can be decomposed as

\[
 M = L_0 + S_0
\]

where $L_0$ is a known matrix of rank $k_0 \leq n$ and $S_0$ a known matrix with a given sparsity $s_0$. Here, we undestand a \textit{sparse matrix} as a matrix containing a lot of zeros and define the ratio between the number of zero-valued entries and the total number of entries as its \textit{sparsity}. In the process of data generation, we follow a reverse approach by constructing first the matrices $L_0$ and $S_0$ with the required properties and merge it together to a matrix $M$ which has by definition the pursued decomposition.\\

For the construction of the low-rank matrix $L_0$, we collect $nk_0$ samples of independent standard normal random variables into an $n$-by-$k_0$ matrix $U$ and set $L_0 = UU^T$. This construction scheme guarantees symmetry and positive semidefinitness of $L_0$ as well as rank $L_0$ = $k_0$.\\

To construct the symmetric positive semidefinite sparse matrix $S_0$, we first take a sample of a uniformly random pair $(i,j)$ with $1 \leq i < j \leq n$ that defines four non-zero entries of an $n$-by-$n$ matrix $\tilde{S_0}$. The off-diagonal elements $(i,j)$ and $(j,i)$ are set to a value $b$ as the realization of a uniformly random variable in $[-1,1]$ while the diagonal elements $(i,i)$ and $(j,j)$ are set to a value $a$ as the realization of a uniformly random variable in $[\vert b \vert,1]$. Due to its construction $\tilde{S_0}$ is positive semidefinite. Finally, we receive the matrix $S_0$ by summing the realizations $\tilde{S_0}^{(i,j)}$ over the pairs $(i,j)$ until the pursued sparsity is reached.\\

According to the described approach, we define a class \textit{SyntheticMatrixSet} which is by its methods able to create a randomly generated matrix $M$ together with $L_0$ and $S_0$ as both parts of its decomposition  $M = L_0 + S_0$ based on the userspecified arguments dimension $n$, rank $k_0$ and sparsity $s_0$. This class enables the generation of highly extensive datasets for various training purposes.

