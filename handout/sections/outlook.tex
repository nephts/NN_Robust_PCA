\newpage
\section{Conclusion and further work}\label{sec:outlook}

In this project we studied a neural network approach for finding a low-rank approximation of a given, partially corrupted, positive semi-definite matrix $M$ (e.g. a covariance matrix of observed data) into low-rank $k$ matrix $L = UU^T$ and sparse matrix $S$ containing the corrupted matrix-entries:
\[
M = L + S \,.
\]
This decomposition solves the problem of \textit{robust principal component analysis}, which essentially breaks down to the search of $k$ approximate eigenvectors of $M$ describing most of the observed variance in the data. The problem is tackled by learning the map $M \mapsto U$ via a deep neural network, in the spirit of \cite{herrera2020denise}. This approach proved to yield a fertile methodology to estimate the low-rank approximations $L$ in comparison to state-of-the-art algorithms such as \textit{Principal component pursuit}. Furthermore, if the network is already trained it out-performs these standard algorithms in terms of speed.

We further generalized the methodology of \cite{herrera2020denise}, to the \textit{robust singular value decomposition} (RSVD) of an arbitrary (non-symmetric) matrix $M = UV^T + S$ into low-rank approximation $L = UV^T$ and sparsity matrix $S$ containing the corrupted entries. This is accomplished through a \textit{collaborative network} ansatz simultaneously training two networks, one to approximate each of the mappings $M\mapsto U, M\mapsto V$. Training both networks in an alternating manner, proved to yield a reasonable trade-off between exploration of the loss landscape and convergence to a minimum. The principal feasibility of this approach was demonstrated on synthetic test sets of small matrices.

In future work it would be very exciting to see how the RSVD performs on realistic data sets of large matrices. Singular value decomposition is one of the major tools in unsupervised machine learning protocols for pre-processing and compression of real world data. Hence, fast and reliable tools to solve this problem, which are additionally stable to noise and corruptions in the data, are of substantial interest in modern applications.

One drawback of the presented neural network approach to PCP, respectively SVD, is the need to know the low rank $k$ by which one wants to approximate the decomposition previous to the design and training of the neural network. Typically, one justifies an effective lower rank $k<n$ of a $n\times n$-matrix by calculating eigenvalues and observing that only a few, say $k$, of them contribute substantially while the remaining eigenvalues are very small. It would be very interesting to explore possible network architectures to solve the RPCA similar to the presented methodology, but which do not require a predefined, target rank. One way would be to incorporate the rank of the approximation as an additional parameter to be learned. Finding in this way an \textit{optimal} low rank, which allows for a best low-rank estimate, would widen the range of possible applications significantly.

