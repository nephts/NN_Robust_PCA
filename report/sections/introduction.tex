\section{Introduction}

In Principal Component Analysis (PCA) we aim at finding the principal components of a set of data points. The principal components of a set of points in $\mathbb{R}^n$ are a sequence of $n$ vectors. They are recursively defined as the $i^{th}$ vector being the direction of a line that best fits the data while being orthogonal to the first $i-1$ vectors. The line is obtained through minimizing its average squared distance from the data points. Intuitively, one can think of PCA as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. Mathematically this corresponds to the eigenvectors of the datas' covariance matrix. Hence, the principal components form an orthonormal basis of the space $\mathbb{R}^n$. PCA is heavily used as an exploratory tool in data analysis. One big application is in quantitative finance (or generally time series analysis), where one is interested in the principal components of the empirical covariance matrix of certain financial assets. Focusing only on the largest principal components, which still hold the most important information about the data, leads to a dimensional reduction of the large dataset such that an efficient analysis of the data is still feasible.

In this project we specifically look at a modification of PCA, which is the Robust Principal Component Analysis. Here our aim is to recover a low rank Matrix $L$ from, possibly highly, corrupted matrices $M$, in the sense that some matrix entries are faulty, for example through imprecise measurements. More precisely, we start with a data matrix (or its covariance matrix) $M \in \mathbb{R}^{m \times n}$ with corrupted entries and attempt to find a decomposition into a sum $M = L + S$. $L$ is a matrix of low rank while the corrupted entries are filtered out into a sparse matrix $S$ (which means that a lot of entries are zero). One contribution of our project is to generalize the network approach of RPCA for symmetric positive semi-definite matrices $M$, to the \textit{robust singular value decomposition} (RSVD) of arbitrary (non-quadratic) matrices.

The usual state-of-the-art algorithms for (R)PCA are computationally demanding and, hence, impractical in some applications like finance, where instantaneous calculation might be required. Therefore, the use of a neuronal network is seen to yield a valuable alternative for the design an efficient decomposition tool. In this project our aim is to implement and generalize the algorithm "Denise" (see \cite{herrera2020denise}) that aims at solving the robust PCA through direct learning of the decomposition map $M\mapsto L+S$ via a deep neural network. We train Denise on a randomly generated synthetic dataset and evaluate the performance of the deep neural network on synthetic and real-world covariance matrices. The advantage of a neural network compared to the standard methods is that once trained the neural network delivers a function that outputs the desired decomposition of a dataset instantaneously, where using standard methods like convex optimization are computationally expensive. The algorithms are matrix specific (meaning that one has to use the algorithm for every new data), which is, therefore, very slow especially when data becomes large. For example in finance, one needs robust low rank estimation of covariance matrices of hundreds of assets instantaneously where the high performance of a neural network is very practical. As argued in \cite{herrera2020denise}, the results achieved by Denise are comparable to several state-of-the-art algorithms in terms of decomposition quality but outperforms all existing algorithms by computation time. As explained above, this is achieved by learning a single evaluation function that takes a matrix as an input and outputs the desired decomposition.

In section \ref{sec:algorithm} the considered algorithm is explained in detail. Especially we introduce the objective function on which we train the neural network and describe the network architecture. Subsequently in section \ref{sec:comparison} we introduce the principal component pursuit (PCP) algorithm, which will be used as benchmark to asses the performance of the neural networks approach. We evaluate PCP as well as the trained Denise network on a Portfolio correlation matrix of five exemplary stocks in the DAX 30 and compare the resulting decompositions. Finally, in section \ref{sec:outlook}, we summarize approaches on how to develop our project further, and which additional tasks we would like to carry out in the course of this semester.





